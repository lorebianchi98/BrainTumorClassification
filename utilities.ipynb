{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "utilities.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/lorebianchi98/BrainTumorClassification/blob/main/utilities.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Utilities and Constants\n",
        "In this notebook we define some constants and utility functions that are used in several notebooks of this project."
      ],
      "metadata": {
        "id": "Nn5J488bZ-ZI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Import"
      ],
      "metadata": {
        "id": "Mxrh0buil6YW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from keras import layers\n",
        "import matplotlib.pyplot as plt\n",
        "from tqdm import tqdm\n",
        "import shutil\n",
        "from tensorflow.keras import optimizers\n",
        "import random as rn\n",
        "import os\n",
        "from sklearn import metrics\n",
        "from sklearn.metrics import ConfusionMatrixDisplay"
      ],
      "metadata": {
        "id": "JbTF2vBXl8OO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Path Costants and Classes\n",
        "This constants keep the values of the paths to the drive folders and local folders used."
      ],
      "metadata": {
        "id": "8exKdDQucau4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "DRIVE_DIR = \"/content/gdrive/MyDrive/BrainTumorClassifier\"\n",
        "DATA_DIR = '/content/brain-tumor-mri-dataset'\n",
        "PREPROCESSED_DIR = '/content/brain-tumor-mri-dataset-cleaned'\n",
        "SETS_DIR = '/content/brain-tumor-mri-splits'\n",
        "MODELS_PATH = '/content/gdrive/MyDrive/BrainTumorClassifier/Models'\n",
        "\n",
        "SEED = 123\n",
        "\n",
        "CLASSES = ['glioma', 'meningioma', 'notumor', 'pituitary']"
      ],
      "metadata": {
        "id": "-6ZCbxPHaNXe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "IMAGE_WIDTH = 224\n",
        "IMAGE_HEIGHT = 224\n",
        "IMAGE_SIZE = (IMAGE_HEIGHT, IMAGE_WIDTH)\n",
        "BATCH_SIZE = 64"
      ],
      "metadata": {
        "id": "_BBSEYkbXBg1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Set Seed\n",
        "In order to obtain reproducible results we set the seeds."
      ],
      "metadata": {
        "id": "monaB3oHTv7V"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "np.random.seed(SEED)\n",
        "rn.seed(SEED)\n",
        "tf.random.set_seed(SEED)"
      ],
      "metadata": {
        "id": "pjpyMkVSUEQ5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Object Handling Utilities"
      ],
      "metadata": {
        "id": "uS4XMw_6TvrB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#provides functions to store and load objects from files \n",
        "import pickle\n",
        "\n",
        "def saveObject(obj, path):\n",
        "    \"\"\"\"Save an object using the pickle library on a file\n",
        "    \n",
        "    :param obj: undefined. Object to save\n",
        "    :param fileName: str. Name of the file of the object to save\n",
        "    \"\"\"\n",
        "    print(\"Saving \" + path + '.pkl')\n",
        "    with open(path + \".pkl\", 'wb') as fid:\n",
        "        pickle.dump(obj, fid)\n",
        "    \n",
        "def loadObject(path):\n",
        "    \"\"\"\"Load an object from a file\n",
        "    \n",
        "    :param fileName: str. Name of the file of the object to load\n",
        "    :return: obj: undefined. Object loaded\n",
        "    \"\"\"\n",
        "    try:\n",
        "        with open(path + '.pkl', 'rb') as fid:\n",
        "            obj = pickle.load(fid)\n",
        "            return obj\n",
        "    except IOError:\n",
        "        return None"
      ],
      "metadata": {
        "id": "azaAYkEVTz7l"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Training Utility\n",
        "Functions used in order to support the training of models"
      ],
      "metadata": {
        "id": "hMSFy8k9crqS"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cfnATN7CiLdW"
      },
      "outputs": [],
      "source": [
        "def load_data_splits (img_size, batch_size, shuffle_on_val=True):\n",
        "  train_ds = tf.keras.preprocessing.image_dataset_from_directory(\n",
        "    SETS_DIR + '/train',\n",
        "    labels='inferred', #the label of the dataset is obtained by the name of the directory\n",
        "    seed=SEED,\n",
        "    shuffle=True,\n",
        "    image_size=img_size,\n",
        "    batch_size=batch_size,\n",
        "  )\n",
        "  val_ds = tf.keras.preprocessing.image_dataset_from_directory(\n",
        "    SETS_DIR + '/val',\n",
        "    labels='inferred', #the label of the dataset is obtained by the name of the directory\n",
        "    seed=SEED,\n",
        "    shuffle=shuffle_on_val,\n",
        "    image_size=img_size,\n",
        "    batch_size=batch_size,\n",
        "  )\n",
        "  test_ds = tf.keras.preprocessing.image_dataset_from_directory(\n",
        "    SETS_DIR + '/test',\n",
        "    labels='inferred', #the label of the dataset is obtained by the name of the directory\n",
        "    seed=SEED,\n",
        "    shuffle=False, \n",
        "    image_size=img_size,\n",
        "    batch_size=batch_size,\n",
        "  )\n",
        "  return train_ds, val_ds, test_ds\n",
        "\n",
        "  \n",
        "def compile_model(model, metrics='accuracy', loss='sparse_categorical_crossentropy', optimizer='adam', learning_rate = 0.0005):\n",
        "  '''\n",
        "    compile_model is used to compile the current model\n",
        "    :param model: model to compile\n",
        "    :param optimizer: optimizer to be used\n",
        "    :param learning_rate: learning rate parameter for the optimizer\n",
        "  '''\n",
        "  if optimizer == 'adam':\n",
        "    optimizer=optimizers.Adam(learning_rate=learning_rate)\n",
        "  elif optimizer == 'rmsprop':\n",
        "    optimizer = optimizers.RMSprop(learning_rate=learning_rate)\n",
        "  else:\n",
        "    return\n",
        "\n",
        "  model.compile(loss=loss,\n",
        "    optimizer=optimizer,\n",
        "    metrics=[metrics])\n",
        "\n",
        "  return model\n",
        "\n",
        "def run_model (model, model_name, train_ds, val_ds, epochs=20, patience=3, monitor='val_loss'):\n",
        "  '''\n",
        "  run_model is used to run the current mode\n",
        "  :param model: model to run\n",
        "  :param model_name: name given to save the model\n",
        "  :param epochs: how many epochs to do\n",
        "  :param patience: patience value for Early Stopping\n",
        "  :param monitor: what to monitor for Early Stopping and Model Checkpoint\n",
        "  '''\n",
        "  # local save path for the models\n",
        "  local_path = 'model/' + model_name + '.h5'\n",
        "  drive_path = MODELS_PATH + '/' + model_name\n",
        "  #deletes old model\n",
        "  try:\n",
        "    shutil.rmtree(drive_path)\n",
        "  except:\n",
        "    pass\n",
        "  os.mkdir(drive_path)\n",
        "  callbacks_list = [\n",
        "                  keras.callbacks.EarlyStopping(monitor=monitor, patience=patience), #we implement EarlyStopping to prevent overfitting\n",
        "                  keras.callbacks.ModelCheckpoint(\n",
        "                      filepath = local_path,\n",
        "                      monitor=monitor,\n",
        "                      verbose=1,\n",
        "                      save_best_only=True)\n",
        "                  ]\n",
        "  history = model.fit(train_ds,\n",
        "                    epochs=epochs,\n",
        "                    validation_data=val_ds,\n",
        "                    callbacks=callbacks_list)\n",
        "  # save on Drive only the best model\n",
        "  shutil.copy(local_path, drive_path + '/' + model_name + '.h5')\n",
        "  # save on Drive also the history\n",
        "  saveObject(history, drive_path + '/history') \n",
        "  return tf.keras.models.load_model(local_path), history\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "In order to fight overfitting we provide keras layers that performs data augmentation:"
      ],
      "metadata": {
        "id": "-GoZ6s37TASp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "data_augmentation_layers = keras.Sequential(\n",
        "  [\n",
        "  layers.RandomFlip(\"horizontal\"), # Applies horizontal flipping to a random 50% of the images\n",
        "  layers.RandomContrast(0.15), # Randomly adjust the contrast of an image or images by a random factor in the range[–15%, +15%] \n",
        "  ]\n",
        ")"
      ],
      "metadata": {
        "id": "47tzMz0bTLgS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Evaluation Utilities\n",
        "Functions used in order to evaluate models."
      ],
      "metadata": {
        "id": "yajBXV43c485"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def plot_accuracy_and_loss_history(history):\n",
        "  acc = history.history['accuracy']\n",
        "  val_acc = history.history['val_accuracy']\n",
        "  loss = history.history['loss']\n",
        "  val_loss = history.history['val_loss']\n",
        "\n",
        "  epochs = range(len(acc))\n",
        "\n",
        "  plt.plot(epochs, acc, 'bo', label='Training accuracy')\n",
        "  plt.plot(epochs, val_acc, 'b', label='Validation accuracy')\n",
        "  plt.title('Training and validation accuracy')\n",
        "  plt.legend()\n",
        "\n",
        "  plt.figure()\n",
        "\n",
        "  plt.plot(epochs, loss, 'bo', label='Training loss')\n",
        "  plt.plot(epochs, val_loss, 'b', label='Validation loss')\n",
        "  plt.title('Training and validation loss')\n",
        "  plt.legend()\n",
        "\n",
        "  plt.show()\n",
        "\n",
        "def evaluate_model(model, test_ds):\n",
        "  test_loss, test_accuracy = model.evaluate(test_ds)\n",
        "  print(\"Loss on test set: \" +str(test_loss))\n",
        "  print(\"Accuracy on test set: \" +str(test_accuracy))\n",
        "  test_labels = np.concatenate([label for image, label in test_ds], axis=0) # retrieve the labels of the test set\n",
        "  test_predictions = model.predict(test_ds)  # probabilities for all classes\n",
        "  test_predictions = np.argmax(test_predictions, axis=-1)  # index of the classes with largest probability\n",
        "  print(metrics.classification_report(test_labels, test_predictions, target_names=CLASSES, digits=4))\n",
        "\n",
        "def plot_confusionmatrix(model, test_ds):\n",
        "  test_labels = np.concatenate([label for image, label in test_ds], axis=0) # retrieve the labels of the test set\n",
        "  test_predictions = model.predict(test_ds, verbose=1)  # probabilities for all classes\n",
        "  test_predictions = np.argmax(test_predictions, axis=-1)  # index of the classes with largest probability\n",
        "  # plot confusion matrix\n",
        "  fig, ax = plt.subplots(figsize=(10, 10))\n",
        "  ConfusionMatrixDisplay.from_predictions(test_labels, test_predictions, xticks_rotation='vertical', ax=ax, display_labels=CLASSES)\n"
      ],
      "metadata": {
        "id": "LQo_ItZ-bz5-"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}